{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classic ML model applied to BoW sequence representation\n"
      ],
      "metadata": {
        "id": "1tGxwf0fp96S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ixZIkvE9j1v5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "dyGJ_hYj2O65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8rOS4bjnBc",
        "outputId": "8b63d497-5069-45a4-a83c-83eef8e50e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read processed data"
      ],
      "metadata": {
        "id": "pggyi6GVsLwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/dev_filtered.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/test_filtered.csv')\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/train_filtered.csv')"
      ],
      "metadata": {
        "id": "3t_gH5RNU9Gh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0Dy-3DypU9kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BoW as a Data Encoding"
      ],
      "metadata": {
        "id": "uWbAVKaCsPsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "egcHmGdZsDT6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizes the sequences into individual characters\n",
        "cv = CountVectorizer(lowercase=False, analyzer='char')"
      ],
      "metadata": {
        "id": "wuwURHOFuC_C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn the vocabulary and set indices for amino acids that are used to generate vector encodings\n",
        "cv.fit(train_df['sequence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "BAthvbfrtF3Z",
        "outputId": "ae8dd6fe-9406-4347-991a-4d5412c26c45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='char', lowercase=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, lowercase=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, lowercase=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsdMUKum0V8P",
        "outputId": "25513e49-35bd-423a-c649-5ce2a8b101e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F': 5,\n",
              " 'V': 19,\n",
              " 'S': 16,\n",
              " 'G': 6,\n",
              " 'A': 0,\n",
              " 'T': 17,\n",
              " 'Q': 14,\n",
              " 'I': 8,\n",
              " 'N': 12,\n",
              " 'E': 4,\n",
              " 'K': 9,\n",
              " 'L': 10,\n",
              " 'D': 3,\n",
              " 'M': 11,\n",
              " 'R': 15,\n",
              " 'H': 7,\n",
              " 'Y': 22,\n",
              " 'C': 2,\n",
              " 'P': 13,\n",
              " 'W': 20,\n",
              " 'X': 21,\n",
              " 'U': 18,\n",
              " 'B': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_vocabulary = len(cv.get_feature_names_out())\n",
        "print(size_vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Q9ReajBVmN",
        "outputId": "f7228094-e6cc-440f-c937-803f4cc19027"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TULxVFcc0jX5",
        "outputId": "28466524-3acc-4fcd-fd63-0e35a2b40649"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N',\n",
              "       'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode amino acids using BoW technique\n",
        "X_train = cv.transform(train_df['sequence'])\n",
        "X_dev = cv.transform(dev_df['sequence'])\n",
        "X_test = cv.transform(test_df['sequence'])"
      ],
      "metadata": {
        "id": "HoQnxjZa007S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example"
      ],
      "metadata": {
        "id": "hNV4mpU9tL23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soc2JGx-1N5S",
        "outputId": "eb8a30f9-c025-4c54-b845-2c0954705704"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360946, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cotg0QWp1N1-",
        "outputId": "7c2391b1-537f-4fc6-9610-78033bf794f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x23 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 20 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_vect = np.array(X_train[0].todense())[0]\n",
        "feat_vect.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOwXQNEP1NyI",
        "outputId": "d34890e1-40d7-48cf-a386-78f11379cd50"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_vect\n",
        "\n",
        "# Feature vector corresponding to the 1st training sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rBJL8AH1NuO",
        "outputId": "8e3fdb74-4ec6-4bbf-9580-ffd91db7015e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15,  0,  2,  9, 12,  5, 10,  2, 13,  7, 21,  1, 14,  1,  8,  2, 10,\n",
              "       13,  0, 17,  1,  0,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[0]['sequence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "K1FH8yIV1Nkl",
        "outputId": "c87d70c7-3cfa-4477-cd10-c787f4d36e22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FVVSGSATQITANQIEKLEEADEFENLLVINLDMKTVLAGVSDEIVNRIVNNLGQNNIVVVHTSTLVRDFDGFSEDSLNAELTKANLANVITDFLAELTQKVVAQKELILITLGGETSYKCCSAIGATQLQLIDEVAPAIALSLDHNAQWIVTKSGNLGGVNTL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'F']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRo20VAP2PAQ",
        "outputId": "e339dbdc-762e-439a-d824-36845048d72b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['F', 'F', 'F', 'F', 'F']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'V']\n",
        "\n",
        "num_ocurrs_v = len([amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'V'])\n",
        "\n",
        "print(f' At index {cv.vocabulary_[\"V\"]}: the number of occurrences \"V\" in input sequence is: {num_ocurrs_v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmnFEyZQ2O9o",
        "outputId": "cc6a1629-b201-427f-ce6c-0a8ca58930f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " At index 19: the number of occurrences \"V\" in input sequence is: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iX5t5lG6D0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get target labels \n",
        "train_labels = train_df[\"true_label_encoded\"].values\n",
        "test_labels = test_df[\"true_label_encoded\"].values\n",
        "dev_labels = dev_df[\"true_label_encoded\"].values"
      ],
      "metadata": {
        "id": "DP1poWrE6Wgx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(train_labels))\n",
        "num_classes"
      ],
      "metadata": {
        "id": "R9PSdXwvukUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fee3fa7-eff7-407f-d2c8-57a3fbac4048"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "896"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DataLoader"
      ],
      "metadata": {
        "id": "-_WxWOGN4UV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Define a custom dataset class that will be used to load the data\n",
        "\"\"\"\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, encoded_sequences, labels):\n",
        "        self.sequences = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.int64)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.sequences[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)"
      ],
      "metadata": {
        "id": "PZsgPIS62O4U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ProteinDataset(X_train.todense(), train_labels)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "bzZHFZ9W2OzR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_ds = ProteinDataset(X_dev.todense(), dev_labels)\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dataset=dev_ds,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "IsqVMjOF2Owf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = ProteinDataset(X_test.todense(), test_labels)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "fFDNdNvJ2Oq9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Regression Classifier"
      ],
      "metadata": {
        "id": "VegwdFof9SBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wPyF-ORrCCRT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(torch.nn.Module):\n",
        "    \"\"\" Naive Softmax regression module \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim  (int): input vector size.\n",
        "            output_dim (int): output vector size (number of classes).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass x through a parameterized linear transformation\n",
        "        y = self.linear(x)\n",
        "\n",
        "        # pass the result through softmax over the last dimension to generate\n",
        "        # a probability distribution vector over the classes:\n",
        "        y = torch.nn.functional.softmax(y, dim=-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "HFSbGHm29UWm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and moving it to the GPU\n",
        "model = SoftmaxRegression(size_vocabulary, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "-6wcpL_99ULt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe311d1-aca7-4492-9508-f6bc0ac4e7e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftmaxRegression(\n",
              "  (linear): Linear(in_features=23, out_features=896, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the DataLoader\n",
        "# for batch_ind, (inputs, labels) in enumerate(train_loader):\n",
        "#     break"
      ],
      "metadata": {
        "id": "y_ymevlK2Olu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "cymsdg5v1NVG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    # num_sample: number of samples explored\n",
        "    num_sample = 0.\n",
        "\n",
        "    # loss_total, acc_total\n",
        "    # variables to collect overall loss and accuracy\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        # .zero_grad() to clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # computing the model predictions\n",
        "        preds = model(inputs.to(device)) \n",
        "\n",
        "        # computing the loss value for the mini-batch\n",
        "        loss = loss_fn(preds, labels.to(device))\n",
        "\n",
        "        # computing the gradient w.r.t. to parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # update learnable parameters \n",
        "        optimizer.step()\n",
        "\n",
        "        # cummulating loss in loss_total\n",
        "        loss_total += float(loss.detach().item())\n",
        "\n",
        "        # cummulating number of correct classifications in acc_total\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds.detach(), dim=-1) == labels.to(preds.device)).sum()\n",
        "        )\n",
        "\n",
        "    # dividing by total number of visited samples\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "zvRuBt30DF8r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): \n",
        "            data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    num_sample = 0.\n",
        "\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        preds = model(inputs.to(device)).detach()\n",
        "        loss_total += float(loss_fn(preds, labels.to(device)).detach().item())\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds, dim=-1) == labels.to(preds.device)).sum()\n",
        "        )\n",
        "\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "uXMqOPx9DF6N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 10\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQexXofYDF3_",
        "outputId": "eb1eaffb-0f0e-40e4-de78-9d4681faf5c3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/10] train.loss: 0.0528 train.acc: 5.39% val.loss: 0.0527 val.acc: 7.36%\n",
            "[EPOCH:  2/10] train.loss: 0.0526 train.acc: 7.77% val.loss: 0.0526 val.acc: 8.03%\n",
            "[EPOCH:  3/10] train.loss: 0.0526 train.acc: 8.19% val.loss: 0.0526 val.acc: 8.25%\n",
            "[EPOCH:  4/10] train.loss: 0.0525 train.acc: 8.38% val.loss: 0.0526 val.acc: 8.42%\n",
            "[EPOCH:  5/10] train.loss: 0.0525 train.acc: 8.51% val.loss: 0.0526 val.acc: 8.55%\n",
            "[EPOCH:  6/10] train.loss: 0.0525 train.acc: 8.68% val.loss: 0.0525 val.acc: 8.79%\n",
            "[EPOCH:  7/10] train.loss: 0.0525 train.acc: 8.86% val.loss: 0.0525 val.acc: 8.86%\n",
            "[EPOCH:  8/10] train.loss: 0.0525 train.acc: 8.93% val.loss: 0.0525 val.acc: 8.93%\n",
            "[EPOCH:  9/10] train.loss: 0.0525 train.acc: 8.97% val.loss: 0.0525 val.acc: 8.99%\n",
            "[EPOCH: 10/10] train.loss: 0.0525 train.acc: 9.01% val.loss: 0.0525 val.acc: 9.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nntw7SYRDF1K",
        "outputId": "f7a257f4-a707-4aa5-ab89-5ef7dd2d3106"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.0525 test.acc: 8.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: The loss on the training and validation datasets stays the same and the accuracy on the training dataset does increase slightly and on the validation dataset also slightly increases. \n",
        "\n",
        "The model suffers from underfitting.  \n",
        "\n",
        "Next steps to do:\n",
        "- Try more epochs for training\n",
        "- Increase complexity of the model (add more FC layers prior to softmax)\n",
        "- Have better hand-crafted feature representations of the protein sequence \n",
        "- Switch to sequence models (to learn features without applying domain knowledge)"
      ],
      "metadata": {
        "id": "_7S1xJi3XNPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCSoftmaxRegression(torch.nn.Module):\n",
        "    \"\"\" Softmax regression module with two hidden layers (dims hardcoded) \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim  (int): input vector size.\n",
        "            output_dim (int): output vector size (number of classes).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 256)\n",
        "        self.fc3 = torch.nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass x through a parameterized linear transformation\n",
        "        # y = self.linear(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        # pass through second hidden layer\n",
        "        x = self.fc2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        # pass through the third (output) layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "\n",
        "        # pass the result through softmax over the last dimension to generate\n",
        "        # a probability distribution vector over the classes:\n",
        "        y = torch.nn.functional.softmax(x, dim=-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "4AzADhwNephI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and moving it to the GPU (if available):\n",
        "model = FCSoftmaxRegression(size_vocabulary, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2V6D7UEiEsH",
        "outputId": "9615626a-0191-40e8-a13c-1abf3b5ca4d6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FCSoftmaxRegression(\n",
              "  (fc1): Linear(in_features=23, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=896, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             weight_decay=0.001\n",
        "                             )"
      ],
      "metadata": {
        "id": "b3n9Kb9ciJEp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 30\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7jFbklEiQUq",
        "outputId": "306f06e7-db1e-4128-cfcd-e8ab7574f1ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/30] train.loss: 0.0526 train.acc: 7.52% val.loss: 0.0525 val.acc: 8.77%\n",
            "[EPOCH:  2/30] train.loss: 0.0524 train.acc: 9.09% val.loss: 0.0524 val.acc: 9.50%\n",
            "[EPOCH:  3/30] train.loss: 0.0524 train.acc: 9.49% val.loss: 0.0524 val.acc: 9.86%\n",
            "[EPOCH:  4/30] train.loss: 0.0524 train.acc: 9.88% val.loss: 0.0524 val.acc: 10.08%\n",
            "[EPOCH:  5/30] train.loss: 0.0523 train.acc: 10.30% val.loss: 0.0524 val.acc: 10.49%\n",
            "[EPOCH:  6/30] train.loss: 0.0523 train.acc: 10.69% val.loss: 0.0523 val.acc: 10.94%\n",
            "[EPOCH:  7/30] train.loss: 0.0523 train.acc: 10.86% val.loss: 0.0523 val.acc: 10.99%\n",
            "[EPOCH:  8/30] train.loss: 0.0523 train.acc: 11.04% val.loss: 0.0523 val.acc: 11.18%\n",
            "[EPOCH:  9/30] train.loss: 0.0523 train.acc: 11.19% val.loss: 0.0523 val.acc: 11.29%\n",
            "[EPOCH: 10/30] train.loss: 0.0522 train.acc: 11.26% val.loss: 0.0523 val.acc: 11.22%\n",
            "[EPOCH: 11/30] train.loss: 0.0522 train.acc: 11.34% val.loss: 0.0523 val.acc: 11.33%\n",
            "[EPOCH: 12/30] train.loss: 0.0522 train.acc: 11.41% val.loss: 0.0523 val.acc: 11.70%\n",
            "[EPOCH: 13/30] train.loss: 0.0522 train.acc: 11.64% val.loss: 0.0523 val.acc: 11.69%\n",
            "[EPOCH: 14/30] train.loss: 0.0522 train.acc: 11.66% val.loss: 0.0523 val.acc: 11.68%\n",
            "[EPOCH: 15/30] train.loss: 0.0522 train.acc: 11.71% val.loss: 0.0523 val.acc: 11.61%\n",
            "[EPOCH: 16/30] train.loss: 0.0522 train.acc: 11.73% val.loss: 0.0523 val.acc: 11.29%\n",
            "[EPOCH: 17/30] train.loss: 0.0522 train.acc: 11.75% val.loss: 0.0523 val.acc: 11.80%\n",
            "[EPOCH: 18/30] train.loss: 0.0522 train.acc: 11.74% val.loss: 0.0523 val.acc: 11.28%\n",
            "[EPOCH: 19/30] train.loss: 0.0522 train.acc: 11.75% val.loss: 0.0523 val.acc: 11.81%\n",
            "[EPOCH: 20/30] train.loss: 0.0522 train.acc: 11.79% val.loss: 0.0523 val.acc: 11.82%\n",
            "[EPOCH: 21/30] train.loss: 0.0522 train.acc: 11.81% val.loss: 0.0523 val.acc: 11.67%\n",
            "[EPOCH: 22/30] train.loss: 0.0522 train.acc: 11.79% val.loss: 0.0523 val.acc: 11.63%\n",
            "[EPOCH: 23/30] train.loss: 0.0522 train.acc: 11.81% val.loss: 0.0523 val.acc: 11.69%\n",
            "[EPOCH: 24/30] train.loss: 0.0522 train.acc: 11.83% val.loss: 0.0523 val.acc: 11.80%\n",
            "[EPOCH: 25/30] train.loss: 0.0522 train.acc: 11.82% val.loss: 0.0523 val.acc: 11.73%\n",
            "[EPOCH: 26/30] train.loss: 0.0522 train.acc: 11.82% val.loss: 0.0523 val.acc: 11.68%\n",
            "[EPOCH: 27/30] train.loss: 0.0522 train.acc: 11.81% val.loss: 0.0523 val.acc: 11.73%\n",
            "[EPOCH: 28/30] train.loss: 0.0522 train.acc: 11.81% val.loss: 0.0523 val.acc: 11.85%\n",
            "[EPOCH: 29/30] train.loss: 0.0522 train.acc: 11.86% val.loss: 0.0523 val.acc: 11.55%\n",
            "[EPOCH: 30/30] train.loss: 0.0522 train.acc: 11.78% val.loss: 0.0523 val.acc: 11.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G2IiimSiVuX",
        "outputId": "a54cdc0a-d8bd-4452-9dcf-253c173f13a5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.0522 test.acc: 11.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: Increasing the model complexity has marginally helped with underfitting problem by increasing accuracy by 2-3%. \n",
        "\n",
        "Hence, to further improve the performance, more complex models can be used over BoW encoding or other known encoding techniques.  \n",
        "\n",
        "Potential improvements:\n",
        "- Use another technique for encoding the protein sequence, e.g. 2-gram BoW, 3-gram BoW, (combination of both)\n",
        "- Use deeper NN (potentially add regularization: L2, dropout)\n",
        "- Hyperparameter tuning, e.g. the default learning rate used (no tuning)"
      ],
      "metadata": {
        "id": "sFSqVinhkc9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = optimizer.param_groups[0]['lr']\n",
        "lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7wvxOcAiWTu",
        "outputId": "2430efa1-c341-4257-a27f-27d90a3ea795"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Predictions"
      ],
      "metadata": {
        "id": "AEjaM2R9eodD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "acc_total = 0.\n",
        "\n",
        "# Keep test predictions\n",
        "test_preds = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    preds = model(inputs.to(device)).detach()\n",
        "    acc_total += float(\n",
        "        (torch.argmax(preds, dim=-1) == labels.to(preds.device)).sum()\n",
        "    )\n",
        "\n",
        "    preds = torch.argmax(preds, dim=-1).cpu().numpy()  # convert to numpy array\n",
        "    test_preds = np.concatenate([test_preds, preds])\n",
        "\n",
        "\n",
        "acc_total = acc_total / (len(test_ds))"
      ],
      "metadata": {
        "id": "jxHBtsV2liIz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJSStzOLe4Ku",
        "outputId": "2d43759e-f87c-4633-9b76-420ae1779ab3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11689329958108381"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save results"
      ],
      "metadata": {
        "id": "4GL9GyHKe-Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['softmaxreg_preds'] = test_preds"
      ],
      "metadata": {
        "id": "qQoQKkLyWy9z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/instaDeep/data/softmaxreg_preds.csv', index=False)"
      ],
      "metadata": {
        "id": "FLZqMQLIW7rm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YL60isqfJYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}