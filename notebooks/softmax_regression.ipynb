{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classic ML model applied to BoW sequence representation\n"
      ],
      "metadata": {
        "id": "1tGxwf0fp96S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ixZIkvE9j1v5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "dyGJ_hYj2O65"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8rOS4bjnBc",
        "outputId": "a59ea717-ec3c-4ac9-87be-5cfb947c113f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read processed data"
      ],
      "metadata": {
        "id": "pggyi6GVsLwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/dev_filtered.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/test_filtered.csv')\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/train_filtered.csv')"
      ],
      "metadata": {
        "id": "3t_gH5RNU9Gh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0Dy-3DypU9kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BoW as a Data Encoding"
      ],
      "metadata": {
        "id": "uWbAVKaCsPsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "egcHmGdZsDT6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizes the sequences into individual characters\n",
        "cv = CountVectorizer(lowercase=False, analyzer='char')"
      ],
      "metadata": {
        "id": "wuwURHOFuC_C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn the vocabulary and set indices for amino acids that are used to generate vector encodings\n",
        "cv.fit(train_df['sequence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "BAthvbfrtF3Z",
        "outputId": "6127347b-9366-4d25-d13b-876e90c08143"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='char', lowercase=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, lowercase=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, lowercase=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsdMUKum0V8P",
        "outputId": "584cd454-ffad-4fb3-a854-863913248c37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F': 5,\n",
              " 'V': 19,\n",
              " 'S': 16,\n",
              " 'G': 6,\n",
              " 'A': 0,\n",
              " 'T': 17,\n",
              " 'Q': 14,\n",
              " 'I': 8,\n",
              " 'N': 12,\n",
              " 'E': 4,\n",
              " 'K': 9,\n",
              " 'L': 10,\n",
              " 'D': 3,\n",
              " 'M': 11,\n",
              " 'R': 15,\n",
              " 'H': 7,\n",
              " 'Y': 22,\n",
              " 'C': 2,\n",
              " 'P': 13,\n",
              " 'W': 20,\n",
              " 'X': 21,\n",
              " 'U': 18,\n",
              " 'B': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_vocabulary = len(cv.get_feature_names_out())\n",
        "print(size_vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1Q9ReajBVmN",
        "outputId": "9f15890e-1420-4770-c272-470fd4d7e0cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TULxVFcc0jX5",
        "outputId": "c5fe6c73-de2e-44bf-cc64-09d285a67d0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N',\n",
              "       'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode amino acids using BoW technique\n",
        "X_train = cv.transform(train_df['sequence'])\n",
        "X_dev = cv.transform(dev_df['sequence'])\n",
        "X_test = cv.transform(test_df['sequence'])"
      ],
      "metadata": {
        "id": "HoQnxjZa007S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example"
      ],
      "metadata": {
        "id": "hNV4mpU9tL23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soc2JGx-1N5S",
        "outputId": "410fb51a-2527-4e06-b942-edbf0d2a4bfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(360946, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cotg0QWp1N1-",
        "outputId": "ff4860fa-397c-4686-fb59-6c32f096b5fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x23 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 20 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_vect = np.array(X_train[0].todense())[0]\n",
        "feat_vect.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOwXQNEP1NyI",
        "outputId": "26a1772f-70ea-4059-8552-fc6e01d52a64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_vect\n",
        "\n",
        "# Feature vector corresponding to the 1st training sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rBJL8AH1NuO",
        "outputId": "c4b083b4-0808-4765-f2e0-5a64c7fa9392"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15,  0,  2,  9, 12,  5, 10,  2, 13,  7, 21,  1, 14,  1,  8,  2, 10,\n",
              "       13,  0, 17,  1,  0,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[0]['sequence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "K1FH8yIV1Nkl",
        "outputId": "69e7272b-282a-4b73-c387-6d6d6b1b9553"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FVVSGSATQITANQIEKLEEADEFENLLVINLDMKTVLAGVSDEIVNRIVNNLGQNNIVVVHTSTLVRDFDGFSEDSLNAELTKANLANVITDFLAELTQKVVAQKELILITLGGETSYKCCSAIGATQLQLIDEVAPAIALSLDHNAQWIVTKSGNLGGVNTL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'F']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRo20VAP2PAQ",
        "outputId": "93196c52-642f-4085-c617-ffd0e8508003"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['F', 'F', 'F', 'F', 'F']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'V']\n",
        "\n",
        "num_ocurrs_v = len([amino_acid for amino_acid in train_df.iloc[0]['sequence'] if amino_acid == 'V'])\n",
        "\n",
        "print(f' At index {cv.vocabulary_[\"V\"]}: the number of occurrences \"V\" in input sequence is: {num_ocurrs_v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmnFEyZQ2O9o",
        "outputId": "ff2a37cc-1c8e-4e18-b3c0-e5c69d152192"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " At index 19: the number of occurrences \"V\" in input sequence is: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iX5t5lG6D0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get target labels \n",
        "train_labels = train_df[\"true_label_encoded\"].values\n",
        "test_labels = test_df[\"true_label_encoded\"].values\n",
        "dev_labels = dev_df[\"true_label_encoded\"].values"
      ],
      "metadata": {
        "id": "DP1poWrE6Wgx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_labels.unique())"
      ],
      "metadata": {
        "id": "R9PSdXwvukUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DataLoader"
      ],
      "metadata": {
        "id": "-_WxWOGN4UV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Define a custom dataset class that will be used to load the data\n",
        "\"\"\"\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, encoded_sequences, labels):\n",
        "        self.sequences = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.int64)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = self.sequences[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)"
      ],
      "metadata": {
        "id": "PZsgPIS62O4U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ProteinDataset(X_train.todense(), train_labels)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "bzZHFZ9W2OzR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_ds = ProteinDataset(X_dev.todense(), dev_labels)\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dataset=dev_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "IsqVMjOF2Owf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = ProteinDataset(X_test.todense(), test_labels)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "fFDNdNvJ2Oq9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Regression Classifier"
      ],
      "metadata": {
        "id": "VegwdFof9SBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wPyF-ORrCCRT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(torch.nn.Module):\n",
        "    \"\"\" Naive Softmax regression module \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim  (int): input vector size.\n",
        "            output_dim (int): output vector size (number of classes).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass x through a parameterized linear transformation\n",
        "        y = self.linear(x)\n",
        "\n",
        "        # pass the result through softmax over the last dimension to generate\n",
        "        # a probability distribution vector over the classes:\n",
        "        y = torch.nn.functional.softmax(y, dim=-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "HFSbGHm29UWm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and moving it to the GPU\n",
        "model = SoftmaxRegression(size_vocabulary, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "-6wcpL_99ULt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the DataLoader\n",
        "# for batch_ind, (inputs, labels) in enumerate(train_loader):\n",
        "#     break"
      ],
      "metadata": {
        "id": "y_ymevlK2Olu"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "cymsdg5v1NVG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    # num_sample: number of samples explored\n",
        "    num_sample = 0.\n",
        "\n",
        "    # loss_total, acc_total\n",
        "    # variables to collect overall loss and accuracy\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        # .zero_grad() to clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # computing the model predictions\n",
        "        preds = model(inputs.to(device)) \n",
        "\n",
        "        # computing the loss value for the mini-batch\n",
        "        loss = loss_fn(preds, labels.to(device))\n",
        "\n",
        "        # computing the gradient w.r.t. to parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # update learnable parameters \n",
        "        optimizer.step()\n",
        "\n",
        "        # cummulating loss in loss_total\n",
        "        loss_total += float(loss.detach().item())\n",
        "\n",
        "        # cummulating number of correct classifications in acc_total\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds.detach(), dim=-1) == torch.argmax(labels.to(device), dim=-1)).sum()\n",
        "        )\n",
        "\n",
        "    # dividing by total number of visited samples\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "zvRuBt30DF8r"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): \n",
        "            data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    num_sample = 0.\n",
        "\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        preds = model(inputs.to(device)).detach()\n",
        "        loss_total += float(loss_fn(preds, labels.to(device)).detach().item())\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds, dim=-1) == torch.argmax(labels.to(device), dim=-1)).sum()\n",
        "        )\n",
        "\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "uXMqOPx9DF6N"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 10\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQexXofYDF3_",
        "outputId": "554ab768-9b1c-46dc-d28b-dc2a3ce1e231"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/10] train.loss: 0.2097 train.acc: 0.15% val.loss: 0.2098 val.acc: 0.20%\n",
            "[EPOCH:  2/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2098 val.acc: 0.15%\n",
            "[EPOCH:  3/10] train.loss: 0.2097 train.acc: 0.14% val.loss: 0.2098 val.acc: 0.15%\n",
            "[EPOCH:  4/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2098 val.acc: 0.19%\n",
            "[EPOCH:  5/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2098 val.acc: 0.17%\n",
            "[EPOCH:  6/10] train.loss: 0.2097 train.acc: 0.14% val.loss: 0.2098 val.acc: 0.15%\n",
            "[EPOCH:  7/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2098 val.acc: 0.15%\n",
            "[EPOCH:  8/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2097 val.acc: 0.17%\n",
            "[EPOCH:  9/10] train.loss: 0.2097 train.acc: 0.12% val.loss: 0.2097 val.acc: 0.16%\n",
            "[EPOCH: 10/10] train.loss: 0.2097 train.acc: 0.13% val.loss: 0.2097 val.acc: 0.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nntw7SYRDF1K",
        "outputId": "9e8f1230-738c-45d4-90e0-179680bb29d7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.2097 test.acc: 0.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: The loss on the training and validation datasets stays the same and the accuracy on the training dataset does not increase and on the validation dataset fluctuates. \n",
        "\n",
        "Next steps to do:\n",
        "- Try more epochs for training\n",
        "- Add regularization (weight decay)\n",
        "- Increase complexity of the model (more FC layers)"
      ],
      "metadata": {
        "id": "_7S1xJi3XNPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and moving it to the GPU (if available):\n",
        "model = SoftmaxRegression(size_vocabulary, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuVmZqenDGqb",
        "outputId": "77c55ef9-83c4-48f6-dd3a-6ad714c9e7c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftmaxRegression(\n",
              "  (linear): Linear(in_features=23, out_features=896, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             weight_decay=0.0001\n",
        "                             )"
      ],
      "metadata": {
        "id": "0OxoXHcrDGnc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 30\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaXCLEgWDGk8",
        "outputId": "cc03821c-6d61-49dd-c355-fa2d58fe5ccf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/30] train.loss: 0.2114 train.acc: 0.09% val.loss: 0.2113 val.acc: 0.29%\n",
            "[EPOCH:  2/30] train.loss: 0.2112 train.acc: 0.09% val.loss: 0.2113 val.acc: 0.13%\n",
            "[EPOCH:  3/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.12%\n",
            "[EPOCH:  4/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.27%\n",
            "[EPOCH:  5/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.15%\n",
            "[EPOCH:  6/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.18%\n",
            "[EPOCH:  7/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.12%\n",
            "[EPOCH:  8/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.13%\n",
            "[EPOCH:  9/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.32%\n",
            "[EPOCH: 10/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.22%\n",
            "[EPOCH: 11/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.18%\n",
            "[EPOCH: 12/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.20%\n",
            "[EPOCH: 13/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.17%\n",
            "[EPOCH: 14/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.15%\n",
            "[EPOCH: 15/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.22%\n",
            "[EPOCH: 16/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.17%\n",
            "[EPOCH: 17/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.21%\n",
            "[EPOCH: 18/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.22%\n",
            "[EPOCH: 19/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.09%\n",
            "[EPOCH: 20/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.13%\n",
            "[EPOCH: 21/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.16%\n",
            "[EPOCH: 22/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.22%\n",
            "[EPOCH: 23/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.19%\n",
            "[EPOCH: 24/30] train.loss: 0.2112 train.acc: 0.12% val.loss: 0.2113 val.acc: 0.19%\n",
            "[EPOCH: 25/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.21%\n",
            "[EPOCH: 26/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.24%\n",
            "[EPOCH: 27/30] train.loss: 0.2112 train.acc: 0.11% val.loss: 0.2113 val.acc: 0.21%\n",
            "[EPOCH: 28/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.20%\n",
            "[EPOCH: 29/30] train.loss: 0.2112 train.acc: 0.14% val.loss: 0.2113 val.acc: 0.20%\n",
            "[EPOCH: 30/30] train.loss: 0.2112 train.acc: 0.13% val.loss: 0.2113 val.acc: 0.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQske1C6DGiK",
        "outputId": "96fd37f2-9966-476b-fc90-12c831f5f292"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.2112 test.acc: 0.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**: Adding L2 regularization does not prevent the val.acc from fluctuating / decreasing. Training for more epochs did not help to overcome issues with low performance on the training dataset. \n",
        "\n",
        "The model suffers from underfitting. \n",
        "Potential improvements:\n",
        "- add more FC layers prior to softmax\n",
        "- have better hand-crafted feature representations of the protein sequence \n",
        "- switch to sequence models (to learn features without applying domain knowledge)"
      ],
      "metadata": {
        "id": "YknF-xYCd8Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCSoftmaxRegression(torch.nn.Module):\n",
        "    \"\"\" Softmax regression module with two hidden layers (dims hardcoded) \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim  (int): input vector size.\n",
        "            output_dim (int): output vector size (number of classes).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 256)\n",
        "        self.fc3 = torch.nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass x through a parameterized linear transformation\n",
        "        # y = self.linear(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        # pass through second hidden layer\n",
        "        x = self.fc2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        # pass through the third (output) layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "\n",
        "        # pass the result through softmax over the last dimension to generate\n",
        "        # a probability distribution vector over the classes:\n",
        "        y = torch.nn.functional.softmax(x, dim=-1)\n",
        "        return y"
      ],
      "metadata": {
        "id": "4AzADhwNephI"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model and moving it to the GPU (if available):\n",
        "model = FCSoftmaxRegression(size_vocabulary, num_classes)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2V6D7UEiEsH",
        "outputId": "139379e9-9d26-4b4e-8c3d-43bee8a9bff2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FCSoftmaxRegression(\n",
              "  (fc1): Linear(in_features=23, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=896, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             weight_decay=0\n",
        "                             )"
      ],
      "metadata": {
        "id": "b3n9Kb9ciJEp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 15\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7jFbklEiQUq",
        "outputId": "212af902-34d6-4693-87f4-8119e4be264e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/15] train.loss: 0.2111 train.acc: 0.10% val.loss: 0.2109 val.acc: 0.06%\n",
            "[EPOCH:  2/15] train.loss: 0.2109 train.acc: 0.23% val.loss: 0.2109 val.acc: 0.43%\n",
            "[EPOCH:  3/15] train.loss: 0.2108 train.acc: 0.22% val.loss: 0.2109 val.acc: 0.09%\n",
            "[EPOCH:  4/15] train.loss: 0.2108 train.acc: 0.18% val.loss: 0.2108 val.acc: 0.35%\n",
            "[EPOCH:  5/15] train.loss: 0.2108 train.acc: 0.20% val.loss: 0.2109 val.acc: 0.19%\n",
            "[EPOCH:  6/15] train.loss: 0.2108 train.acc: 0.22% val.loss: 0.2109 val.acc: 0.28%\n",
            "[EPOCH:  7/15] train.loss: 0.2108 train.acc: 0.16% val.loss: 0.2108 val.acc: 0.45%\n",
            "[EPOCH:  8/15] train.loss: 0.2109 train.acc: 0.14% val.loss: 0.2109 val.acc: 0.17%\n",
            "[EPOCH:  9/15] train.loss: 0.2108 train.acc: 0.22% val.loss: 0.2109 val.acc: 0.33%\n",
            "[EPOCH: 10/15] train.loss: 0.2109 train.acc: 0.34% val.loss: 0.2108 val.acc: 0.30%\n",
            "[EPOCH: 11/15] train.loss: 0.2108 train.acc: 0.20% val.loss: 0.2109 val.acc: 0.09%\n",
            "[EPOCH: 12/15] train.loss: 0.2108 train.acc: 0.15% val.loss: 0.2110 val.acc: 0.44%\n",
            "[EPOCH: 13/15] train.loss: 0.2109 train.acc: 0.22% val.loss: 0.2109 val.acc: 0.15%\n",
            "[EPOCH: 14/15] train.loss: 0.2109 train.acc: 0.22% val.loss: 0.2110 val.acc: 0.18%\n",
            "[EPOCH: 15/15] train.loss: 0.2109 train.acc: 0.17% val.loss: 0.2110 val.acc: 0.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G2IiimSiVuX",
        "outputId": "84c2abb5-63fe-43a6-a835-65193a129e1e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.2109 test.acc: 0.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: Increasing the model complexity has helped with underfitting problem. Hence, to further improve the performance, more complex models can be used over BoW encoding or other known encoding techniques.\n",
        "\n",
        "Potential improvements:\n",
        "- Use another technique for encoding the protein sequence, e.g. 2-gram BoW, 3-gram BoW, (combination of both)\n",
        "- Use deeper NN (potentially add regularization: L2, dropout)\n",
        "- Since the training is not stable, early stopping might be helpful to get the best-performing model\n",
        "- Hyperparameter tuning, e.g. the default learning rate used (no tuning)."
      ],
      "metadata": {
        "id": "sFSqVinhkc9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = optimizer.param_groups[0]['lr']\n",
        "lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7wvxOcAiWTu",
        "outputId": "2c927504-a175-4588-bd97-fe53bfad2aec"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxHBtsV2liIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}