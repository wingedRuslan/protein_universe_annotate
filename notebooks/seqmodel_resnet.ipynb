{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet as a sequence model applied to protein dataset"
      ],
      "metadata": {
        "id": "1tGxwf0fp96S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ixZIkvE9j1v5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "dyGJ_hYj2O65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8rOS4bjnBc",
        "outputId": "e0e5e1a2-186f-410c-bc20-5e0015c0a953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read processed data"
      ],
      "metadata": {
        "id": "pggyi6GVsLwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/dev_filtered.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/test_filtered.csv')\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/instaDeep/data/train_filtered.csv')"
      ],
      "metadata": {
        "id": "3t_gH5RNU9Gh"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 100"
      ],
      "metadata": {
        "id": "Gn2Eg3E_uall"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0Dy-3DypU9kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot Encoding"
      ],
      "metadata": {
        "id": "uWbAVKaCsPsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "jVikPiseqkCD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')"
      ],
      "metadata": {
        "id": "gUT-zeMXqj-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_seqs = np.concatenate((dev_df['sequence'].values, test_df['sequence'].values, train_df['sequence'].values))"
      ],
      "metadata": {
        "id": "hrZ2SoyyrJmP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(all_seqs) == len(dev_df) + len(test_df) + len(train_df)"
      ],
      "metadata": {
        "id": "8psUGJYwrqdg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = (''.join(seq) for seq in all_seqs)"
      ],
      "metadata": {
        "id": "NDMdeUyGrPCG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = np.array(list(''.join(X_all)), dtype=str)"
      ],
      "metadata": {
        "id": "dmgYOkpasCLF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfUCb_NrsK7i",
        "outputId": "586d52b0-6983-4dfc-c460-59a469acef13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53388595,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape the sequence data to a shape of N×1 \n",
        "# to make the OneHotEncoder learn a single encoding \n",
        "# that maps each amino-acid symbol to its respective one-hot vector\n",
        "X_all = X_all.reshape(-1,1)"
      ],
      "metadata": {
        "id": "ksB-wmRksKye"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder.fit(X_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "qubu-KNfsfA4",
        "outputId": "96bfe1f2-0502-4885-c412-1ecf3c48dae7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(handle_unknown='ignore')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(one_hot_encoder.get_feature_names_out())\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo2jDzcVse9C",
        "outputId": "89c21efc-9287-44e7-d767-33b1de4ba3c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_to_one_hot_encoding(seq, encoder):\n",
        "    oht = list(seq)\n",
        "    oht = np.array(oht).reshape(-1,1)\n",
        "    oht = encoder.transform(oht).toarray()\n",
        "    return oht"
      ],
      "metadata": {
        "id": "JXPIs5VrsKtt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idx = np.random.randint(0, 100)\n",
        "idx = 62\n",
        "sequence = train_df['sequence'].values[idx]\n",
        "print(f'Sequence = {sequence}')\n",
        "\n",
        "matrix = sequence_to_one_hot_encoding(sequence, one_hot_encoder)\n",
        "print(f'Matrix   = ')\n",
        "for line in matrix[:5]: # show 5 first lines\n",
        "    print(line)\n",
        "print('...')\n",
        "for line in matrix[-5:]:# and 5 last lines\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISe0JNf7qj2C",
        "outputId": "ab8504e5-dc01-4ecf-9853-d58de2a6f1c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence = VDIQIFGRTLRINCPDEEKSYLKKASENLEKRLINLKEKSKISNTEQLLFIAALNMCYELDKEKNKFKKYSINIEKCIKLLKNTI\n",
            "Matrix   = \n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "...\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8WgwpSqjyH",
        "outputId": "110b2c8e-d7e5-4805-9b53-e6a09cf283d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpNO5aSWqjuB",
        "outputId": "144f0b63-ba34-4bf8-bdf1-84404cf61ca0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_to_one_hot_encoding(seq, encoder):\n",
        "    # converting to an array of characters\n",
        "    seq = np.array(list(seq))\n",
        "\n",
        "    # generating the one-hot encoding matrix\n",
        "    oht = encoder.transform(seq.reshape(-1, 1))\n",
        "    oht = oht.toarray()\n",
        "\n",
        "    return oht"
      ],
      "metadata": {
        "id": "SAAn7oAxuKrc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_processing(seq, seq_len):\n",
        "\n",
        "    # Truncate\n",
        "    if seq_len > MAX_SEQ_LEN:\n",
        "        seq = seq[ : MAX_SEQ_LEN]\n",
        "\n",
        "    # Padding the sequence to the maximum length\n",
        "    seq = seq.ljust(MAX_SEQ_LEN)\n",
        "\n",
        "    # One-hot Encoding\n",
        "    encoded_seq = sequence_to_one_hot_encoding(seq, one_hot_encoder)\n",
        "    \n",
        "    # Set padded part to 0s to indicate end of sequence\n",
        "    if seq_len < MAX_SEQ_LEN:\n",
        "        encoded_seq[seq_len:] = 0\n",
        "    \n",
        "    return encoded_seq"
      ],
      "metadata": {
        "id": "IAAFHlrDtO-V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.iloc[idx][['sequence', 'seq_length']]\n",
        "\n",
        "test_seq = train_df.iloc[idx]['sequence']\n",
        "test_seq_len = train_df.iloc[idx]['seq_length']\n",
        "\n",
        "test_seq_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yjcH0xHtOym",
        "outputId": "42872468-4bd2-40cc-8a92-2a27a9ea0895"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_test_seq = one_hot_processing(test_seq, test_seq_len)"
      ],
      "metadata": {
        "id": "aA5eUjtNtOp_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_test_seq[85]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whu_I_Q1tOla",
        "outputId": "55dae502-3d7d-4b44-f485-049e24be7b5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_test_seq[84]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpgAl9dFqjph",
        "outputId": "851342d2-6143-4015-b4f3-0400a5d9d51a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the encoding \n",
        "\n",
        "Apply the one-hot encoding to each sequence in the DataFrame"
      ],
      "metadata": {
        "id": "cUtNjdUhxDja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_encoded_seqs = np.array([one_hot_processing(seq, seq_len) \n",
        "                            for (seq, seq_len) in zip(dev_df['sequence'], dev_df['seq_length'])])"
      ],
      "metadata": {
        "id": "Rnqron7ayP3S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoded_seqs = np.array([one_hot_processing(seq, seq_len) \n",
        "                             for (seq, seq_len) in zip(test_df['sequence'], test_df['seq_length'])])"
      ],
      "metadata": {
        "id": "0yGtNyyLyRJM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = train_df.head(250000)\n",
        "train_df = train_df.sample(n=250000, random_state=42)"
      ],
      "metadata": {
        "id": "F7ILkpeL3oUs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded_seqs = np.array([one_hot_processing(seq, seq_len) \n",
        "                              for (seq, seq_len) in zip(train_df['sequence'], train_df['seq_length'])])"
      ],
      "metadata": {
        "id": "l8r4zr8DyepM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the encoded sequences as a numpy array\n",
        "np.save('./dev_encoded_seqs.npy', dev_encoded_seqs)\n",
        "np.save('./test_encoded_seqs.npy', test_encoded_seqs)\n",
        "np.save('./train_encoded_seqs.npy', train_encoded_seqs)"
      ],
      "metadata": {
        "id": "g9GC40edytpw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get target labels \n",
        "train_labels = train_df[\"true_label_encoded\"].values\n",
        "test_labels = test_df[\"true_label_encoded\"].values\n",
        "dev_labels = dev_df[\"true_label_encoded\"].values"
      ],
      "metadata": {
        "id": "Q0SliHKe0rig"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DataLoader"
      ],
      "metadata": {
        "id": "-_WxWOGN4UV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Define a custom dataset class that will be used to load the data\n",
        "\"\"\"\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, encoded_sequences, labels):\n",
        "        self.sequences = torch.tensor(encoded_sequences, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.int64)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # x = self.sequences[index]\n",
        "        x = self.sequences[index].unsqueeze(0) # add a new dimension for the channel\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)"
      ],
      "metadata": {
        "id": "PZsgPIS62O4U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ProteinDataset(train_encoded_seqs, train_labels)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "bzZHFZ9W2OzR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_ds[0]"
      ],
      "metadata": {
        "id": "rzfMBA-oA1-4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2ltApKSA7um",
        "outputId": "ebd873a0-ddb1-479d-9743-aeb038e7beb6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_ds = ProteinDataset(dev_encoded_seqs, dev_labels)\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dataset=dev_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "IsqVMjOF2Owf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = ProteinDataset(test_encoded_seqs, test_labels)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "fFDNdNvJ2Oq9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the DataLoader\n",
        "for batch_ind, (inputs, labels) in enumerate(train_loader):\n",
        "    break"
      ],
      "metadata": {
        "id": "y_ymevlK2Olu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIqP9yxC0-TI",
        "outputId": "7efeeb52-2432-4265-961f-9b27d5f0ed74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 100, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet model as in ProtCNN"
      ],
      "metadata": {
        "id": "VegwdFof9SBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "gllYNP3e7mUk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wPyF-ORrCCRT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ResNet18 model with modified layers\n",
        "model = models.resnet18(pretrained=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8H4gegu7m-n",
        "outputId": "60239933-49f7-4bdd-f7ac-2a856bbc6cd5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 896"
      ],
      "metadata": {
        "id": "cWvmtLbq8eww"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.unique(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snv9FCRPMuQ9",
        "outputId": "55a65245-411a-4028-d5b8-eaff711b49c8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "896"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Modify the first convolutional layer to accept input with 100 channels\n",
        "# model.conv1 = nn.Conv2d(100, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "# model.conv1 = nn.Conv2d(1, 64, kernel_size=8, stride=2, padding=3, bias=False)\n",
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "\n",
        "# Modify the average pooling layer to output a tensor with shape (1, 1)\n",
        "model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "# Modify the last fully connected layer of the ResNet18 model \n",
        "# by changing the number of output features from the default 1000 to <num_classes>\n",
        "model.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjL-jsSg7wY8",
        "outputId": "8f3efe0a-fef6-48f1-c4c7-be5cd1456a27"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=896, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "a3Y7IKEkJJE8"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             lr=learning_rate,\n",
        "                             )"
      ],
      "metadata": {
        "id": "iL7cUBvb8nM0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = optimizer.param_groups[0]['lr']\n",
        "learning_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EESeJLcvI2j5",
        "outputId": "b814e77e-bdc3-4c6a-886d-4e608c301607"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging\n",
        "# for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
        "#     break\n",
        "\n",
        "# inputs.shape\n",
        "\n",
        "# labels.shape\n",
        "\n",
        "# preds = model(inputs.to(device))\n",
        "\n",
        "# preds.shape\n",
        "\n",
        "# preds = torch.argmax(preds.detach(), dim=-1)\n",
        "\n",
        "# preds.shape\n",
        "\n",
        "# labels = labels.to(preds.device)\n",
        "\n",
        "# (preds == labels).sum()"
      ],
      "metadata": {
        "id": "CJ0CkRiEPjbh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    # num_sample: number of samples explored\n",
        "    num_sample = 0.\n",
        "\n",
        "    # loss_total, acc_total\n",
        "    # variables to collect overall loss and accuracy\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # for inputs, labels in dataloader:\n",
        "    for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        # .zero_grad() to clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # computing the model predictions\n",
        "        preds = model(inputs.to(device)) \n",
        "\n",
        "        # computing the loss value for the mini-batch\n",
        "        loss = loss_fn(preds, labels.to(device))\n",
        "\n",
        "        # computing the gradient w.r.t. to parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # update learnable parameters \n",
        "        optimizer.step()\n",
        "\n",
        "        # cummulating loss in loss_total\n",
        "        loss_total += float(loss.detach().item())\n",
        "\n",
        "        # cummulating number of correct classifications in acc_total\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds.detach(), dim=-1) == labels.to(preds.device)).sum()\n",
        "        )\n",
        "        \n",
        "        # Display the current progress using tqdm\n",
        "        if i % 400 == 0:\n",
        "            train_acc = 100 * acc_total / num_sample\n",
        "            tqdm.write(f'Batch {i}, Loss: {loss.item():.4f}, Accuracy: {train_acc:.2f}%')\n",
        "\n",
        "    # dividing by total number of visited samples\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "zvRuBt30DF8r"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, loss_fn, optimizer):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model  (torch.nn.Module): model.\n",
        "        dataloader (torch.utils.data.DataLoader): \n",
        "            data loader object to use for training.\n",
        "    Returns:\n",
        "        loss_total (float): loss value.\n",
        "        acc_total  (float): accuracy.\n",
        "    \"\"\"\n",
        "    num_sample = 0.\n",
        "\n",
        "    loss_total = 0.\n",
        "    acc_total = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        num_sample += float(inputs.size(0))\n",
        "\n",
        "        preds = model(inputs.to(device)).detach()\n",
        "        loss_total += float(loss_fn(preds, labels.to(device)).detach().item())\n",
        "        acc_total += float(\n",
        "            (torch.argmax(preds, dim=-1) == labels.to(preds.device)).sum()\n",
        "        )\n",
        "\n",
        "    loss_total = loss_total / num_sample\n",
        "    acc_total = acc_total / num_sample\n",
        "\n",
        "    return loss_total, acc_total"
      ],
      "metadata": {
        "id": "uXMqOPx9DF6N"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 3\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss_total, train_acc_total = train(model, train_loader, loss_fn, optimizer)\n",
        "    val_loss_total, val_acc_total = evaluate(model, dev_loader, loss_fn, optimizer)\n",
        "\n",
        "    print(f'[EPOCH:{epoch+1:3d}/{max_epochs}]',\n",
        "        f'train.loss: {train_loss_total:.4f}',\n",
        "        f'train.acc: {100*train_acc_total:3.2f}%',\n",
        "        f'val.loss: {val_loss_total:.4f}',\n",
        "        f'val.acc: {100*val_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQexXofYDF3_",
        "outputId": "bf740ae3-db0f-485a-8deb-3b2438bc7720"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 4/1954 [00:00<02:01, 16.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 6.9547, Accuracy: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 403/1954 [00:17<01:07, 23.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 400, Loss: 1.2205, Accuracy: 39.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 805/1954 [00:34<00:50, 22.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 800, Loss: 0.4759, Accuracy: 60.54%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1204/1954 [00:51<00:31, 23.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1200, Loss: 0.3171, Accuracy: 69.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1603/1954 [01:08<00:14, 23.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1600, Loss: 0.2950, Accuracy: 75.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1954/1954 [01:22<00:00, 23.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  1/3] train.loss: 0.0084 train.acc: 78.42% val.loss: 0.0012 val.acc: 92.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2/1954 [00:00<01:41, 19.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 0.1154, Accuracy: 97.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 404/1954 [00:17<01:05, 23.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 400, Loss: 0.0308, Accuracy: 95.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 803/1954 [00:34<00:48, 23.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 800, Loss: 0.1976, Accuracy: 95.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1205/1954 [00:50<00:31, 23.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1200, Loss: 0.1299, Accuracy: 95.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1604/1954 [01:07<00:14, 23.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1600, Loss: 0.1420, Accuracy: 95.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1954/1954 [01:22<00:00, 23.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  2/3] train.loss: 0.0014 train.acc: 95.23% val.loss: 0.0008 val.acc: 95.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/1954 [00:00<01:29, 21.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Loss: 0.0513, Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 405/1954 [00:17<01:06, 23.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 400, Loss: 0.0479, Accuracy: 97.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 804/1954 [00:33<00:48, 23.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 800, Loss: 0.0916, Accuracy: 97.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 1203/1954 [00:50<00:32, 23.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1200, Loss: 0.0871, Accuracy: 97.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 1605/1954 [01:07<00:14, 23.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1600, Loss: 0.1784, Accuracy: 97.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1954/1954 [01:22<00:00, 23.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH:  3/3] train.loss: 0.0008 train.acc: 97.19% val.loss: 0.0007 val.acc: 95.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total, test_acc_total = evaluate(model, test_loader, loss_fn, optimizer)\n",
        "\n",
        "print(f'[Test set performance]',\n",
        "        f'test.loss: {test_loss_total:.4f}',\n",
        "        f'test.acc: {100*test_acc_total:3.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nntw7SYRDF1K",
        "outputId": "1af81904-b0d1-422d-b209-e5ce1cd38071"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test set performance] test.loss: 0.0007 test.acc: 95.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate predictions"
      ],
      "metadata": {
        "id": "mgPXFmC-TivO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "acc_total = 0.\n",
        "\n",
        "# Keep test predictions\n",
        "test_preds = np.empty((0,), dtype=np.int64)\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    preds = model(inputs.to(device)).detach()\n",
        "    acc_total += float(\n",
        "        (torch.argmax(preds, dim=-1) == labels.to(preds.device)).sum()\n",
        "    )\n",
        "\n",
        "    preds = torch.argmax(preds, dim=-1).cpu().numpy()  # convert to numpy array\n",
        "    test_preds = np.concatenate([test_preds, preds])\n",
        "\n",
        "\n",
        "acc_total = acc_total / (len(test_ds))"
      ],
      "metadata": {
        "id": "rOxlp9vqUJAC"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjU1CuHHV4-t",
        "outputId": "1363be50-65dd-4516-ea88-4089c9d4750d"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.956764264432447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_mqt0fzV9a-",
        "outputId": "751cd3ae-c58d-4bd8-fa61-f3f0aeda3b25"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44639"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVWX2bc4WxO9",
        "outputId": "89bb2f3b-de25-4082-a838-a4164a81cf2a"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['resnet_preds'] = test_preds"
      ],
      "metadata": {
        "id": "qQoQKkLyWy9z"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/instaDeep/data/resnet_test_preds.csv', index=False)"
      ],
      "metadata": {
        "id": "FLZqMQLIW7rm"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZMTMxBpXQgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}